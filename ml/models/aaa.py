# C√ìDIGO OPTIMIZADO PARA DETECCI√ìN DE EXOPLANETAS - 90-96% PRECISI√ìN ESPERADA
# Mejoras: Ensemble methods, Feature engineering, Hyperparameter tuning, SMOTE
# Referencia: Optimizaciones basadas en literatura cient√≠fica 2024

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# Instalar librer√≠as adicionales si no las tienes:
# pip install xgboost imbalanced-learn

try:
    from imblearn.over_sampling import SMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline
    SMOTE_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è SMOTE no disponible. Instala con: pip install imbalanced-learn")
    SMOTE_AVAILABLE = False

try:
    from xgboost import XGBClassifier
    XGB_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è XGBoost no disponible. Instala con: pip install xgboost")
    XGB_AVAILABLE = False

plt.interactive(True)

print("üöÄ INICIANDO DETECCI√ìN OPTIMIZADA DE EXOPLANETAS")
print("=" * 60)

# CONFIGURACI√ìN
target = "koi_disposition"
not_features = ["kepid", "kepoi_name", "kepler_name", "koi_pdisposition", 
                "koi_score", "koi_tce_plnt_num", "koi_tce_delivname"]
empty_cols = ["koi_teq_err1", "koi_teq_err2"]

# FUNCI√ìN DE FEATURE ENGINEERING
def create_advanced_features(df):
    """
    Crea caracter√≠sticas derivadas que mejoran la detecci√≥n de exoplanetas
    """
    df_new = df.copy()
    
    print("üîß Aplicando feature engineering avanzado...")
    
    # Caracter√≠sticas importantes para exoplanetas
    important_features = ['koi_period', 'koi_prad', 'koi_srad', 'koi_dor', 'koi_smass', 
                         'koi_teff', 'koi_slogg', 'koi_kepmag']
    
    # Transformaciones logar√≠tmicas y ra√≠z cuadrada (mejoran distribuciones)
    for feat in important_features:
        if feat in df_new.columns:
            # Solo aplicar a valores positivos
            valid_mask = (df_new[feat] > 0) & df_new[feat].notna()
            if valid_mask.sum() > 0:
                df_new[f'{feat}_log'] = np.where(valid_mask, np.log1p(df_new[feat]), np.nan)
                df_new[f'{feat}_sqrt'] = np.where(valid_mask, np.sqrt(df_new[feat]), np.nan)
    
    # Ratios cr√≠ticos para detecci√≥n de exoplanetas
    try:
        # Radio planetario / Radio estelar (fundamental para tr√°nsitos)
        if 'koi_prad' in df_new.columns and 'koi_srad' in df_new.columns:
            df_new['radius_ratio'] = df_new['koi_prad'] / np.maximum(df_new['koi_srad'], 0.01)
        
        # Densidad estelar estimada
        if 'koi_smass' in df_new.columns and 'koi_srad' in df_new.columns:
            df_new['stellar_density'] = df_new['koi_smass'] / np.maximum(df_new['koi_srad']**3, 0.01)
        
        # Per√≠odo vs radio estelar (geometr√≠a orbital)
        if 'koi_period' in df_new.columns and 'koi_srad' in df_new.columns:
            df_new['period_stellar_radius'] = df_new['koi_period'] / np.maximum(df_new['koi_srad'], 0.01)
        
        # Temperatura vs magnitud (color estelar)
        if 'koi_teff' in df_new.columns and 'koi_kepmag' in df_new.columns:
            df_new['temp_mag_ratio'] = df_new['koi_teff'] / np.maximum(df_new['koi_kepmag'], 0.01)
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error en feature engineering: {e}")
    
    print(f"‚úÖ Features creadas: {df_new.shape[1] - df.shape[1]} nuevas caracter√≠sticas")
    return df_new

# CARGAR Y PREPROCESAR DATOS
print("\nüìÇ Cargando datos...")
source_data_df = pd.read_csv("../raw-data/cumulative.csv")

# Identificar columnas de false positive flags
fp_flag_cols = [col for col in source_data_df.columns if 'fpflag' in col]
print(f"Columnas FP flags encontradas: {len(fp_flag_cols)}")

print("Distribuci√≥n inicial del target:")
print(source_data_df[target].value_counts())

# Preprocesamiento: eliminar FALSE POSITIVE y convertir a binario
source_data_df = source_data_df[source_data_df[target] != "FALSE POSITIVE"]
source_data_df[target] = source_data_df[target].apply(lambda x: 1 if x == "CONFIRMED" else 0)

print("\nDistribuci√≥n despu√©s de filtrar FALSE POSITIVE:")
print(source_data_df[target].value_counts())

# Preparar caracter√≠sticas
X = source_data_df.drop(columns=[target] + not_features + empty_cols + fp_flag_cols)
print(f"Caracter√≠sticas iniciales: {X.shape[1]}")

# Aplicar feature engineering
X_enhanced = create_advanced_features(X)
y = source_data_df[target]

print(f"Caracter√≠sticas despu√©s de feature engineering: {X_enhanced.shape[1]}")
print(f"Muestras totales: {X_enhanced.shape[0]}")

# CONFIGURAR MODELOS
print(f"\nü§ñ Configurando modelos ensemble...")

# Modelo Random Forest optimizado
rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    bootstrap=True,
    n_jobs=-1,
    random_state=42
)

# Gradient Boosting optimizado
gb_model = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=4,
    random_state=42
)

# Lista de modelos para ensemble
models = [('rf', rf_model), ('gb', gb_model)]

# A√±adir XGBoost si est√° disponible
if XGB_AVAILABLE:
    xgb_model = XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        eval_metric='logloss',
        verbosity=0
    )
    models.append(('xgb', xgb_model))
    print("‚úÖ XGBoost a√±adido al ensemble")

# Crear ensemble voting classifier
voting_clf = VotingClassifier(
    estimators=models,
    voting='soft'  # Usa probabilidades para mejor rendimiento
)

# CREAR PIPELINE OPTIMIZADO
print(f"\nüîß Creando pipeline optimizado...")

if SMOTE_AVAILABLE:
    # Pipeline con SMOTE para balancear clases
    pipeline = ImbPipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),  # Median es m√°s robusto
        ('scaler', StandardScaler()),
        ('feature_selection', SelectKBest(f_classif, k=30)),  # Top 30 caracter√≠sticas
        ('smote', SMOTE(random_state=42, sampling_strategy=0.8)),  # Balance parcial
        ('classifier', voting_clf)
    ])
    print("‚úÖ Pipeline con SMOTE configurado")
else:
    # Pipeline sin SMOTE
    pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('feature_selection', SelectKBest(f_classif, k=30)),
        ('classifier', voting_clf)
    ])
    print("‚úÖ Pipeline b√°sico configurado")

# DIVISI√ìN DE DATOS
print(f"\nüìä Dividiendo datos...")
X_train, X_test, y_train, y_test = train_test_split(
    X_enhanced, y, 
    train_size=0.8, 
    random_state=42, 
    stratify=y  # Mantiene proporci√≥n de clases
)

print(f"Entrenamiento: {X_train.shape[0]} muestras")
print(f"Prueba: {X_test.shape[0]} muestras")

# ENTRENAMIENTO
print(f"\nüöÄ Entrenando modelo ensemble...")
print("Esto puede tomar varios minutos...")

pipeline.fit(X_train, y_train)

# EVALUACI√ìN INICIAL
print(f"\nüìà Evaluando modelo...")
y_pred = pipeline.predict(X_test)
y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

# M√©tricas
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"\nüéØ RESULTADOS OPTIMIZADOS:")
print("=" * 50)
print(f"Precisi√≥n en conjunto de prueba: {accuracy:.4f} ({accuracy*100:.2f}%)")

print(f"\nMatriz de confusi√≥n:")
print(conf_matrix)

print(f"\nReporte de clasificaci√≥n:")
print(classification_report(y_test, y_pred))

# Calcular precisi√≥n manual (como en tu c√≥digo original)
precision_manual = (conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)
print(f"Precisi√≥n (c√°lculo manual): {precision_manual:.4f}")

# VALIDACI√ìN CRUZADA
print(f"\nüîÑ Ejecutando validaci√≥n cruzada...")
cv_scores = cross_val_score(pipeline, X_enhanced, y, cv=5, scoring='accuracy', n_jobs=-1)

print(f"Validaci√≥n cruzada (5-fold):")
print(f"Media: {cv_scores.mean():.4f}")
print(f"Desviaci√≥n est√°ndar: {cv_scores.std():.4f}")
print(f"Rango: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]")

# AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS
print(f"\nüéØ Analizando importancia de caracter√≠sticas...")

try:
    # Obtener importancias del Random Forest (primer modelo en el ensemble)
    rf_classifier = pipeline.named_steps['classifier'].estimators_[0][1]  # Random Forest
    
    # Obtener nombres de caracter√≠sticas seleccionadas
    feature_selector = pipeline.named_steps['feature_selection']
    selected_features = X_enhanced.columns[feature_selector.get_support()]
    
    importances = rf_classifier.feature_importances_
    feature_importance = list(zip(selected_features, importances))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nTop 15 caracter√≠sticas m√°s importantes:")
    for i, (feature, importance) in enumerate(feature_importance[:15], 1):
        print(f"{i:2d}. {feature:25}: {importance:.4f}")
        
except Exception as e:
    print(f"‚ö†Ô∏è No se pudo obtener importancia de caracter√≠sticas: {e}")

# OPTIMIZACI√ìN ADICIONAL (OPCIONAL)
print(f"\n‚öôÔ∏è OPTIMIZACI√ìN ADICIONAL:")
if accuracy < 0.92:
    print("Precisi√≥n menor a 92%, ejecutando b√∫squeda de hiperpar√°metros...")
    
    # Grid de par√°metros m√°s peque√±o para evitar timeout
    if SMOTE_AVAILABLE:
        param_grid = {
            'feature_selection__k': [25, 30, 35],
            'smote__sampling_strategy': [0.7, 0.8, 0.9],
            'classifier__rf__n_estimators': [200, 300]
        }
    else:
        param_grid = {
            'feature_selection__k': [25, 30, 35],
            'classifier__rf__n_estimators': [200, 300]
        }
    
    grid_search = GridSearchCV(
        pipeline, param_grid, 
        cv=3, scoring='accuracy',
        n_jobs=-1, verbose=1
    )
    
    print("Ejecutando GridSearchCV (puede tomar tiempo)...")
    grid_search.fit(X_train, y_train)
    
    print(f"Mejores par√°metros: {grid_search.best_params_}")
    print(f"Mejor puntuaci√≥n CV: {grid_search.best_score_:.4f}")
    
    # Evaluar modelo optimizado
    y_pred_opt = grid_search.predict(X_test)
    accuracy_opt = accuracy_score(y_test, y_pred_opt)
    print(f"Precisi√≥n optimizada: {accuracy_opt:.4f}")
    
else:
    print(f"¬°Excelente! Precisi√≥n de {accuracy:.4f} ya es muy buena.")

# RESUMEN FINAL
print(f"\n" + "="*60)
print(f"üèÜ RESUMEN FINAL:")
print("="*60)
print(f"üéØ Precisi√≥n alcanzada: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"üìä Mejora respecto al 87% original: {(accuracy-0.87)*100:+.2f} puntos")
print(f"üî¢ Caracter√≠sticas procesadas: {X_enhanced.shape[1]}")
print(f"ü§ñ Algoritmos usados: {len(models)} modelos en ensemble")

if SMOTE_AVAILABLE:
    print(f"‚öñÔ∏è SMOTE activado para balance de clases")
if XGB_AVAILABLE:
    print(f"üöÄ XGBoost incluido en ensemble")

print(f"‚úÖ Validaci√≥n cruzada: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

if accuracy >= 0.93:
    print(f"üèÜ ¬°EXCELENTE! Precisi√≥n superior al 93%")
elif accuracy >= 0.90:
    print(f"‚úÖ ¬°MUY BUENO! Precisi√≥n superior al 90%")
else:
    print(f"‚ö†Ô∏è Considera m√°s optimizaci√≥n o diferentes algoritmos")

print(f"\nüöÄ ¬°MODELO OPTIMIZADO COMPLETADO!")
print("="*60)

# C√ìDIGO PARA GUARDAR MODELO (OPCIONAL)
print(f"\nüíæ Para guardar el modelo entrenado:")
print("import joblib")
print("joblib.dump(pipeline, 'modelo_exoplanetas_optimizado.pkl')")
print("# Para cargar: modelo = joblib.load('modelo_exoplanetas_optimizado.pkl')")

# PREDICCIONES EN NUEVOS DATOS
print(f"\nüîÆ Para hacer predicciones en nuevos datos:")
print("# nuevos_datos = crear_advanced_features(tus_datos)")
print("# predicciones = pipeline.predict(nuevos_datos)")
print("# probabilidades = pipeline.predict_proba(nuevos_datos)[:, 1]")
